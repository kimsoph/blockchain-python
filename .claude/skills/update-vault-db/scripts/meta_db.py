#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ZK-PARA Vault ë©”íƒ€ë°ì´í„° DB ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸
YAML frontmatterë¥¼ íŒŒì‹±í•˜ì—¬ SQLite DBë¡œ ì €ì¥

ì‚¬ìš©ë²•:
    python meta_db.py --init          # DB ì´ˆê¸°í™”
    python meta_db.py --sync          # ì „ì²´ ë™ê¸°í™”
    python meta_db.py --search "ê²€ìƒ‰ì–´"  # ê²€ìƒ‰
    python meta_db.py --info          # DB í†µê³„

í•œê¸€ ì¸ì½”ë”© ì²˜ë¦¬:
    - ëª¨ë“  íŒŒì¼ ê²½ë¡œëŠ” NFC ì •ê·œí™” ì ìš©
    - SQLiteëŠ” UTF-8 ì¸ì½”ë”© ê°•ì œ
    - Windows ì½˜ì†” UTF-8 ì¶œë ¥ ì„¤ì •
"""

import argparse
import io
import json
import os
import re
import sqlite3
import sys
import unicodedata
from datetime import datetime
from pathlib import Path
from typing import Optional

# ============================================================
# í•œê¸€ ì¸ì½”ë”© ì„¤ì • (ìµœìš°ì„  ì‹¤í–‰)
# ============================================================

# í™˜ê²½ ë³€ìˆ˜ ê°•ì œ ì„¤ì • (Python UTF-8 ëª¨ë“œ)
os.environ['PYTHONUTF8'] = '1'
os.environ['PYTHONIOENCODING'] = 'utf-8'

# Windows ì½˜ì†” UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    # ì½˜ì†” ì½”ë“œ í˜ì´ì§€ë¥¼ UTF-8ë¡œ ë³€ê²½ ì‹œë„
    try:
        import subprocess
        subprocess.run(['chcp', '65001'], shell=True, capture_output=True)
    except Exception:
        pass

    # stdout/stderr UTF-8 ë˜í¼
    if hasattr(sys.stdout, 'buffer'):
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    if hasattr(sys.stderr, 'buffer'):
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# ============================================================
# ì„¤ì •
# ============================================================

VAULT_ROOT = Path(__file__).resolve().parents[4]  # ZK-PARA í´ë”
DB_PATH = VAULT_ROOT / ".claude" / "ZK-PARA.db"

# ì œì™¸í•  í´ë”
EXCLUDE_DIRS = {
    '.claude',
    '.git',
    '.obsidian',
    'node_modules',
    '__pycache__',
    '.venv',
    'venv',
}

# í´ë” ì¹´í…Œê³ ë¦¬ ë§¤í•‘
FOLDER_CATEGORY_MAP = {
    '0_Inbox': 'Inbox',
    '1_Projects': 'Projects',
    '2_Areas': 'Areas',
    '3_Resources': 'Resources',
    '4_Archive': 'Archive',
    '5_Zettelkasten': 'Zettelkasten',
    '9_Attachments': 'Attachments',
    '9_Imports': 'Imports',
    '9_Templates': 'Templates',
    '_Docs': 'Docs',
}

# ============================================================
# í•œê¸€ ì¸ì½”ë”© ìœ í‹¸ë¦¬í‹°
# ============================================================

def normalize_path(path_str: str) -> str:
    """
    íŒŒì¼ ê²½ë¡œë¥¼ NFC ì •ê·œí™”.
    Windowsì—ì„œ í•œê¸€ íŒŒì¼ëª…ì˜ ì¡°í•©í˜•(NFD)/ì™„ì„±í˜•(NFC) ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²°.
    macOSëŠ” NFDë¥¼ ì‚¬ìš©í•˜ê³ , Windows/LinuxëŠ” NFCë¥¼ ì‚¬ìš©í•¨.
    """
    return unicodedata.normalize('NFC', path_str)


def normalize_text(text) -> str:
    """í…ìŠ¤íŠ¸ë¥¼ NFC ì •ê·œí™”. ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ', 'ë¡œ ì¡°ì¸"""
    if text is None:
        return None
    if isinstance(text, list):
        # ë¦¬ìŠ¤íŠ¸ ìš”ì†Œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ì¡°ì¸
        text = ', '.join(str(item) for item in text)
    if not isinstance(text, str):
        text = str(text)
    return unicodedata.normalize('NFC', text)


def safe_path_str(path: Path, base: Path) -> str:
    """
    Path ê°ì²´ë¥¼ ì•ˆì „í•œ ë¬¸ìì—´ë¡œ ë³€í™˜.
    - NFC ì •ê·œí™”
    - ë°±ìŠ¬ë˜ì‹œë¥¼ ìŠ¬ë˜ì‹œë¡œ ë³€í™˜
    - ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    """
    rel_path = path.relative_to(base)
    path_str = str(rel_path).replace('\\', '/')
    return normalize_path(path_str)


def read_file_with_fallback(file_path: Path) -> tuple[str, str]:
    """
    ë‹¤ì–‘í•œ ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì½ê¸° ì‹œë„.
    ìˆœì„œ: utf-8 â†’ utf-8-sig â†’ cp949 â†’ euc-kr â†’ utf-8(errors='replace')

    Returns:
        (content, encoding_used) íŠœí”Œ
    """
    encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']

    for enc in encodings:
        try:
            with open(file_path, 'r', encoding=enc) as f:
                content = f.read()
            return content, enc
        except (UnicodeDecodeError, UnicodeError):
            continue

    # ìµœí›„ì˜ ìˆ˜ë‹¨: UTF-8 + ëŒ€ì²´ ë¬¸ì
    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
        return f.read(), 'utf-8-fallback'


def convert_to_utf8(file_path: Path, content: str) -> bool:
    """
    íŒŒì¼ì„ UTF-8ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥.

    Args:
        file_path: ë³€í™˜í•  íŒŒì¼ ê²½ë¡œ
        content: ì´ë¯¸ ì½ì€ íŒŒì¼ ë‚´ìš©

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        with open(file_path, 'w', encoding='utf-8', newline='\n') as f:
            f.write(content)
        return True
    except Exception:
        return False


def get_sqlite_connection(db_path: Path) -> sqlite3.Connection:
    """
    UTF-8 ì¸ì½”ë”©ì´ ë³´ì¥ëœ SQLite ì—°ê²° ìƒì„±.
    """
    conn = sqlite3.connect(db_path)
    # UTF-8 ì¸ì½”ë”© í™•ì¸ (SQLite ê¸°ë³¸ê°’ì´ì§€ë§Œ ëª…ì‹œì  í™•ì¸)
    cursor = conn.cursor()
    cursor.execute("PRAGMA encoding")
    encoding = cursor.fetchone()[0]
    if encoding != 'UTF-8':
        print(f"âš  DB ì¸ì½”ë”©ì´ UTF-8ì´ ì•„ë‹˜: {encoding}")

    # ì™¸ë˜ í‚¤ ì œì•½ í™œì„±í™”
    conn.execute("PRAGMA foreign_keys = ON")
    return conn


# ============================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================

def parse_yaml_frontmatter(content: str) -> tuple[dict, str]:
    """YAML frontmatter íŒŒì‹±. (frontmatter_dict, body) ë°˜í™˜"""
    if not content.startswith('---'):
        return {}, content

    # YAML ë¸”ë¡ ë ì°¾ê¸°
    end_match = re.search(r'\n---\s*\n', content[3:])
    if not end_match:
        return {}, content

    yaml_str = content[3:end_match.start() + 3]
    body = content[end_match.end() + 3:]

    # ê°„ë‹¨í•œ YAML íŒŒì‹± (PyYAML ì—†ì´)
    frontmatter = {}
    current_key = None
    list_values = []

    for line in yaml_str.split('\n'):
        line = line.rstrip()
        if not line or line.startswith('#'):
            continue

        # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ
        if line.startswith('  - ') or line.startswith('- '):
            if current_key:
                value = line.lstrip(' -').strip()
                list_values.append(value)
            continue

        # ì´ì „ ë¦¬ìŠ¤íŠ¸ ì €ì¥
        if current_key and list_values:
            frontmatter[current_key] = list_values
            list_values = []

        # key: value íŒŒì‹±
        if ':' in line:
            key, _, value = line.partition(':')
            key = key.strip()
            value = value.strip()

            # ê°’ ì •ë¦¬
            if value.startswith('"') and value.endswith('"'):
                value = value[1:-1]
            elif value.startswith("'") and value.endswith("'"):
                value = value[1:-1]
            elif value.startswith('[') and value.endswith(']'):
                # ì¸ë¼ì¸ ë°°ì—´
                items = value[1:-1].split(',')
                value = [item.strip().strip('"\'') for item in items if item.strip()]

            if value == '' or value == '[]':
                current_key = key
                list_values = []
            else:
                frontmatter[key] = value
                current_key = None

    # ë§ˆì§€ë§‰ ë¦¬ìŠ¤íŠ¸ ì €ì¥
    if current_key and list_values:
        frontmatter[current_key] = list_values

    return frontmatter, body


def extract_title_from_body(body: str) -> Optional[str]:
    """ë³¸ë¬¸ì—ì„œ ì²« ë²ˆì§¸ í—¤ë”© ì¶”ì¶œ"""
    for line in body.split('\n'):
        line = line.strip()
        if line.startswith('# '):
            return line[2:].strip()
    return None


def extract_wikilinks(content: str) -> list[dict]:
    """ìœ„í‚¤ë§í¬ ì¶”ì¶œ. [[target|display]] ë˜ëŠ” ![[embed]] í˜•ì‹"""
    links = []

    # ì¼ë°˜ ìœ„í‚¤ë§í¬: [[target]] ë˜ëŠ” [[target|display]]
    for match in re.finditer(r'\[\[([^\]|]+)(?:\|([^\]]+))?\]\]', content):
        target = match.group(1).strip()
        display = match.group(2).strip() if match.group(2) else None
        links.append({
            'target': target,
            'display': display,
            'type': 'wikilink'
        })

    # ì„ë² ë“œ: ![[embed]]
    for match in re.finditer(r'!\[\[([^\]|]+)(?:\|[^\]]+)?\]\]', content):
        target = match.group(1).strip()
        links.append({
            'target': target,
            'display': None,
            'type': 'embed'
        })

    return links


def get_folder_category(file_path: Path) -> str:
    """íŒŒì¼ ê²½ë¡œì—ì„œ í´ë” ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
    rel_path = file_path.relative_to(VAULT_ROOT)
    parts = rel_path.parts

    if parts:
        top_folder = parts[0]
        return FOLDER_CATEGORY_MAP.get(top_folder, 'Other')
    return 'Other'


def get_folder_path(file_path: Path) -> str:
    """ìƒìœ„ í´ë” ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ)"""
    rel_path = file_path.relative_to(VAULT_ROOT)
    return str(rel_path.parent).replace('\\', '/')


def count_words(text: str) -> int:
    """ë‹¨ì–´ ìˆ˜ ê³„ì‚° (í•œê¸€ + ì˜ì–´)"""
    # í•œê¸€ì€ ê¸€ì ìˆ˜, ì˜ì–´ëŠ” ë‹¨ì–´ ìˆ˜
    korean = len(re.findall(r'[\uac00-\ud7af]', text))
    english = len(re.findall(r'[a-zA-Z]+', text))
    return korean + english


def get_summary(body: str, max_length: int = 200) -> str:
    """ë³¸ë¬¸ ìš”ì•½ (ì²« Nì)"""
    # í—¤ë”©ê³¼ ë¹ˆ ì¤„ ì œê±°
    lines = []
    for line in body.split('\n'):
        line = line.strip()
        if line and not line.startswith('#'):
            lines.append(line)
        if len(' '.join(lines)) > max_length:
            break

    summary = ' '.join(lines)[:max_length]
    if len(summary) == max_length:
        summary = summary.rsplit(' ', 1)[0] + '...'
    return summary


# ============================================================
# DB ìŠ¤í‚¤ë§ˆ
# ============================================================

SCHEMA_SQL = """
-- ë…¸íŠ¸ ë©”íƒ€ë°ì´í„°
CREATE TABLE IF NOT EXISTS notes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    file_path TEXT UNIQUE NOT NULL,
    file_name TEXT NOT NULL,
    title TEXT,
    author TEXT,
    note_type TEXT,
    source TEXT,
    status TEXT,
    created TEXT,
    updated TEXT,
    date_consumed TEXT,
    folder_category TEXT,
    folder_path TEXT,
    summary TEXT,
    word_count INTEGER,
    has_frontmatter INTEGER DEFAULT 0,
    frontmatter_raw TEXT,
    indexed_at TEXT
);

-- íƒœê·¸
CREATE TABLE IF NOT EXISTS tags (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    tag_name TEXT UNIQUE NOT NULL
);

-- ë…¸íŠ¸-íƒœê·¸ ê´€ê³„
CREATE TABLE IF NOT EXISTS note_tags (
    note_id INTEGER NOT NULL,
    tag_id INTEGER NOT NULL,
    PRIMARY KEY (note_id, tag_id),
    FOREIGN KEY (note_id) REFERENCES notes(id) ON DELETE CASCADE,
    FOREIGN KEY (tag_id) REFERENCES tags(id) ON DELETE CASCADE
);

-- ë…¸íŠ¸ ê°„ ë§í¬
CREATE TABLE IF NOT EXISTS links (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source_path TEXT NOT NULL,
    target_note TEXT NOT NULL,
    link_type TEXT DEFAULT 'wikilink',
    display_text TEXT,
    FOREIGN KEY (source_path) REFERENCES notes(file_path) ON DELETE CASCADE
);

-- ë©”íƒ€ ì •ë³´
CREATE TABLE IF NOT EXISTS meta_info (
    key TEXT PRIMARY KEY,
    value TEXT,
    updated_at TEXT
);

-- ì¸ë±ìŠ¤
CREATE INDEX IF NOT EXISTS idx_notes_folder_category ON notes(folder_category);
CREATE INDEX IF NOT EXISTS idx_notes_author ON notes(author);
CREATE INDEX IF NOT EXISTS idx_notes_note_type ON notes(note_type);
CREATE INDEX IF NOT EXISTS idx_notes_status ON notes(status);
CREATE INDEX IF NOT EXISTS idx_links_source ON links(source_path);
CREATE INDEX IF NOT EXISTS idx_links_target ON links(target_note);
"""


# ============================================================
# DB ì‘ì—…
# ============================================================

def init_db():
    """DB ì´ˆê¸°í™” (ìŠ¤í‚¤ë§ˆ ìƒì„±)"""
    DB_PATH.parent.mkdir(parents=True, exist_ok=True)

    conn = get_sqlite_connection(DB_PATH)
    conn.executescript(SCHEMA_SQL)

    # ë©”íƒ€ ì •ë³´ ì´ˆê¸°í™”
    now = datetime.now().isoformat()
    conn.execute(
        "INSERT OR REPLACE INTO meta_info (key, value, updated_at) VALUES (?, ?, ?)",
        ('schema_version', '1.0', now)
    )
    conn.execute(
        "INSERT OR REPLACE INTO meta_info (key, value, updated_at) VALUES (?, ?, ?)",
        ('vault_root', str(VAULT_ROOT), now)
    )

    conn.commit()
    conn.close()
    print(f"âœ“ DB ì´ˆê¸°í™” ì™„ë£Œ: {DB_PATH}")


def sync_all(verbose: bool = False):
    """
    ì „ì²´ ë™ê¸°í™” (YAML â†’ DB)

    Args:
        verbose: Trueë©´ ê° íŒŒì¼ ì²˜ë¦¬ ìƒí™© ì¶œë ¥
    """
    if not DB_PATH.exists():
        print("DBê°€ ì—†ìŠµë‹ˆë‹¤. --initìœ¼ë¡œ ë¨¼ì € ì´ˆê¸°í™”í•˜ì„¸ìš”.")
        return

    conn = get_sqlite_connection(DB_PATH)
    cursor = conn.cursor()

    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
    cursor.execute("DELETE FROM note_tags")
    cursor.execute("DELETE FROM links")
    cursor.execute("DELETE FROM notes")
    # tagsëŠ” ìœ ì§€ (ì¬ì‚¬ìš©)

    now = datetime.now().isoformat()
    note_count = 0
    link_count = 0
    error_count = 0
    tag_cache = {}  # tag_name -> tag_id

    # ì¸ì½”ë”© ê´€ë ¨ ì¶”ì  ë³€ìˆ˜
    error_details: list[tuple[str, str, str]] = []  # (file_path, error_type, message)
    encoding_conversions: list[tuple[str, str]] = []  # (file_path, original_encoding)
    conversion_failures: list[tuple[str, str]] = []  # (file_path, error_message)

    # ê¸°ì¡´ íƒœê·¸ ë¡œë“œ
    for row in cursor.execute("SELECT id, tag_name FROM tags"):
        tag_cache[normalize_text(row[1])] = row[0]

    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìˆœíšŒ
    md_files = list(VAULT_ROOT.rglob('*.md'))
    total_files = len(md_files)
    print(f"ğŸ“‚ ë°œê²¬ëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼: {total_files}ê°œ")

    for idx, md_file in enumerate(md_files, 1):
        # ì œì™¸ í´ë” ì²´í¬
        rel_path = md_file.relative_to(VAULT_ROOT)
        if any(part in EXCLUDE_DIRS for part in rel_path.parts):
            continue

        try:
            # íŒŒì¼ ì½ê¸° (ë‹¤ì¤‘ ì¸ì½”ë”© fallback)
            content, encoding_used = read_file_with_fallback(md_file)

            # BOM ì œê±° (í˜¹ì‹œ ë‚¨ì•„ìˆì„ ê²½ìš°)
            if content.startswith('\ufeff'):
                content = content[1:]

            # ë‚´ìš© NFC ì •ê·œí™”
            content = normalize_text(content)

            # UTF-8 ì™¸ ì¸ì½”ë”© íŒŒì¼ì€ ìë™ ë³€í™˜
            if encoding_used not in ('utf-8', 'utf-8-sig'):
                if convert_to_utf8(md_file, content):
                    encoding_conversions.append((str(rel_path), encoding_used))
                else:
                    conversion_failures.append((str(rel_path), f"ë³€í™˜ ì‹¤íŒ¨ (ì›ë³¸: {encoding_used})"))
        except Exception as e:
            error_details.append((str(rel_path), 'file_access', str(e)))
            error_count += 1
            if verbose:
                print(f"  âš  ì½ê¸° ì‹¤íŒ¨ [{idx}/{total_files}]: {rel_path} - {e}")
            continue

        # YAML íŒŒì‹±
        frontmatter, body = parse_yaml_frontmatter(content)
        has_frontmatter = 1 if frontmatter else 0

        # ì œëª© ì¶”ì¶œ (NFC ì •ê·œí™”)
        title = frontmatter.get('title')
        if not title:
            title = extract_title_from_body(body)
        if not title:
            title = md_file.stem
        title = normalize_text(str(title))

        # ë…¸íŠ¸ ë°ì´í„° ì¤€ë¹„ (ê²½ë¡œëŠ” safe_path_strë¡œ ì •ê·œí™”)
        file_path_str = safe_path_str(md_file, VAULT_ROOT)
        file_name = normalize_path(md_file.name)

        note_data = {
            'file_path': file_path_str,
            'file_name': file_name,
            'title': title,
            'author': normalize_text(frontmatter.get('author')),
            'note_type': normalize_text(frontmatter.get('type')),
            'source': normalize_text(frontmatter.get('source')),
            'status': normalize_text(frontmatter.get('status')),
            'created': str(frontmatter.get('created', '')),
            'updated': str(frontmatter.get('updated', '')),
            'date_consumed': str(frontmatter.get('date_consumed', '')),
            'folder_category': get_folder_category(md_file),
            'folder_path': get_folder_path(md_file),
            'summary': get_summary(body),
            'word_count': count_words(content),
            'has_frontmatter': has_frontmatter,
            'frontmatter_raw': json.dumps(frontmatter, ensure_ascii=False) if frontmatter else None,
            'indexed_at': now,
        }

        # ë…¸íŠ¸ ì‚½ì…
        cursor.execute("""
            INSERT INTO notes (
                file_path, file_name, title, author, note_type, source, status,
                created, updated, date_consumed, folder_category, folder_path,
                summary, word_count, has_frontmatter, frontmatter_raw, indexed_at
            ) VALUES (
                :file_path, :file_name, :title, :author, :note_type, :source, :status,
                :created, :updated, :date_consumed, :folder_category, :folder_path,
                :summary, :word_count, :has_frontmatter, :frontmatter_raw, :indexed_at
            )
        """, note_data)

        note_id = cursor.lastrowid
        note_count += 1

        # íƒœê·¸ ì²˜ë¦¬ (NFC ì •ê·œí™” ì ìš©)
        tags = frontmatter.get('tags', [])
        if isinstance(tags, str):
            tags = [tags]

        for tag_name in tags:
            tag_name = normalize_text(str(tag_name).strip())
            if not tag_name:
                continue

            # íƒœê·¸ ID ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ìƒì„±)
            if tag_name not in tag_cache:
                cursor.execute("INSERT OR IGNORE INTO tags (tag_name) VALUES (?)", (tag_name,))
                cursor.execute("SELECT id FROM tags WHERE tag_name = ?", (tag_name,))
                tag_cache[tag_name] = cursor.fetchone()[0]

            tag_id = tag_cache[tag_name]
            cursor.execute(
                "INSERT OR IGNORE INTO note_tags (note_id, tag_id) VALUES (?, ?)",
                (note_id, tag_id)
            )

        # ë§í¬ ì¶”ì¶œ (íƒ€ê²Ÿ ê²½ë¡œë„ NFC ì •ê·œí™”)
        links = extract_wikilinks(content)
        for link in links:
            target = normalize_text(link['target'])
            display = normalize_text(link['display']) if link['display'] else None
            cursor.execute("""
                INSERT INTO links (source_path, target_note, link_type, display_text)
                VALUES (?, ?, ?, ?)
            """, (file_path_str, target, link['type'], display))
            link_count += 1

        # ì§„í–‰ ìƒí™© ì¶œë ¥ (verbose ëª¨ë“œ ë˜ëŠ” 100ê°œë§ˆë‹¤)
        if verbose or (idx % 100 == 0):
            print(f"  ì²˜ë¦¬ ì¤‘: {idx}/{total_files} - {file_name}")

    # ë©”íƒ€ ì •ë³´ ì—…ë°ì´íŠ¸
    cursor.execute(
        "INSERT OR REPLACE INTO meta_info (key, value, updated_at) VALUES (?, ?, ?)",
        ('last_sync', now, now)
    )
    cursor.execute(
        "INSERT OR REPLACE INTO meta_info (key, value, updated_at) VALUES (?, ?, ?)",
        ('note_count', str(note_count), now)
    )
    cursor.execute(
        "INSERT OR REPLACE INTO meta_info (key, value, updated_at) VALUES (?, ?, ?)",
        ('link_count', str(link_count), now)
    )

    conn.commit()
    conn.close()

    print(f"\nâœ“ ë™ê¸°í™” ì™„ë£Œ")
    print(f"  - ë…¸íŠ¸: {note_count}ê°œ")
    print(f"  - ë§í¬: {link_count}ê°œ")
    print(f"  - íƒœê·¸: {len(tag_cache)}ê°œ")

    # UTF-8 ìë™ ë³€í™˜ ê²°ê³¼ ì¶œë ¥
    if encoding_conversions:
        print(f"\nâœ“ UTF-8 ìë™ ë³€í™˜ ({len(encoding_conversions)}ê°œ):")
        for file_path, enc in encoding_conversions[:10]:
            print(f"    - {file_path} ({enc} â†’ UTF-8)")
        if len(encoding_conversions) > 10:
            print(f"    ... ì™¸ {len(encoding_conversions) - 10}ê°œ")

    # ë³€í™˜ ì‹¤íŒ¨ ì¶œë ¥
    if conversion_failures:
        print(f"\nâš  UTF-8 ë³€í™˜ ì‹¤íŒ¨ ({len(conversion_failures)}ê°œ):")
        for file_path, msg in conversion_failures[:10]:
            print(f"    - {file_path}: {msg}")
        if len(conversion_failures) > 10:
            print(f"    ... ì™¸ {len(conversion_failures) - 10}ê°œ")

    # ì˜¤ë¥˜ ìƒì„¸ ì¶œë ¥
    if error_details:
        print(f"\nâš  ì²˜ë¦¬ ì‹¤íŒ¨ íŒŒì¼ ({len(error_details)}ê°œ):")
        for file_path, error_type, msg in error_details[:10]:
            print(f"    - [{error_type}] {file_path}: {msg}")
        if len(error_details) > 10:
            print(f"    ... ì™¸ {len(error_details) - 10}ê°œ")


def search(keyword: str):
    """í‚¤ì›Œë“œ ê²€ìƒ‰"""
    if not DB_PATH.exists():
        print("DBê°€ ì—†ìŠµë‹ˆë‹¤. --initìœ¼ë¡œ ë¨¼ì € ì´ˆê¸°í™”í•˜ì„¸ìš”.")
        return

    # ê²€ìƒ‰ì–´ NFC ì •ê·œí™”
    keyword = normalize_text(keyword)

    conn = get_sqlite_connection(DB_PATH)
    cursor = conn.cursor()

    query = """
        SELECT file_path, title, author, folder_category, note_type
        FROM notes
        WHERE title LIKE ? OR author LIKE ? OR summary LIKE ?
        ORDER BY folder_category, title
    """
    pattern = f'%{keyword}%'
    cursor.execute(query, (pattern, pattern, pattern))

    results = cursor.fetchall()
    conn.close()

    if not results:
        print(f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return

    print(f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê±´\n")
    for row in results:
        file_path, title, author, category, note_type = row
        author_str = f" - {author}" if author else ""
        type_str = f" [{note_type}]" if note_type else ""
        print(f"  [{category}] {title}{author_str}{type_str}")
        print(f"    â†’ {file_path}")


def search_by_tag(tag_name: str):
    """íƒœê·¸ë¡œ ê²€ìƒ‰"""
    if not DB_PATH.exists():
        print("DBê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # íƒœê·¸ëª… NFC ì •ê·œí™”
    tag_name = normalize_text(tag_name)

    conn = get_sqlite_connection(DB_PATH)
    cursor = conn.cursor()

    query = """
        SELECT n.file_path, n.title, n.author, n.folder_category
        FROM notes n
        JOIN note_tags nt ON n.id = nt.note_id
        JOIN tags t ON nt.tag_id = t.id
        WHERE t.tag_name = ?
        ORDER BY n.folder_category, n.title
    """
    cursor.execute(query, (tag_name,))

    results = cursor.fetchall()
    conn.close()

    if not results:
        print(f"íƒœê·¸ '{tag_name}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return

    print(f"íƒœê·¸ '{tag_name}' ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê±´\n")
    for row in results:
        file_path, title, author, category = row
        author_str = f" - {author}" if author else ""
        print(f"  [{category}] {title}{author_str}")
        print(f"    â†’ {file_path}")


def search_by_category(category: str):
    """í´ë” ì¹´í…Œê³ ë¦¬ë¡œ ê²€ìƒ‰"""
    if not DB_PATH.exists():
        print("DBê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì¹´í…Œê³ ë¦¬ NFC ì •ê·œí™”
    category = normalize_text(category)

    conn = get_sqlite_connection(DB_PATH)
    cursor = conn.cursor()

    query = """
        SELECT file_path, title, author, note_type
        FROM notes
        WHERE folder_category = ?
        ORDER BY title
    """
    cursor.execute(query, (category,))

    results = cursor.fetchall()
    conn.close()

    if not results:
        print(f"ì¹´í…Œê³ ë¦¬ '{category}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return

    print(f"ì¹´í…Œê³ ë¦¬ '{category}' ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê±´\n")
    for row in results:
        file_path, title, author, note_type = row
        author_str = f" - {author}" if author else ""
        type_str = f" [{note_type}]" if note_type else ""
        print(f"  {title}{author_str}{type_str}")
        print(f"    â†’ {file_path}")


def execute_query(query: str):
    """ì§ì ‘ ì¿¼ë¦¬ ì‹¤í–‰"""
    if not DB_PATH.exists():
        print("DBê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    conn = get_sqlite_connection(DB_PATH)
    cursor = conn.cursor()

    try:
        cursor.execute(query)
        results = cursor.fetchall()

        if cursor.description:
            headers = [desc[0] for desc in cursor.description]
            print(" | ".join(headers))
            print("-" * (len(" | ".join(headers))))

            for row in results:
                print(" | ".join(str(v) if v else "" for v in row))

        print(f"\nì´ {len(results)}ê±´")
    except Exception as e:
        print(f"ì¿¼ë¦¬ ì˜¤ë¥˜: {e}")
    finally:
        conn.close()


def show_info():
    """DB í†µê³„ ì •ë³´"""
    if not DB_PATH.exists():
        print("DBê°€ ì—†ìŠµë‹ˆë‹¤. --initìœ¼ë¡œ ë¨¼ì € ì´ˆê¸°í™”í•˜ì„¸ìš”.")
        return

    conn = get_sqlite_connection(DB_PATH)
    cursor = conn.cursor()

    print("=" * 50)
    print("ZK-PARA ë©”íƒ€DB í†µê³„")
    print("=" * 50)

    # ê¸°ë³¸ ì •ë³´
    print(f"\nDB ìœ„ì¹˜: {DB_PATH}")
    print(f"DB í¬ê¸°: {DB_PATH.stat().st_size / 1024:.1f} KB")

    # ë©”íƒ€ ì •ë³´
    cursor.execute("SELECT key, value, updated_at FROM meta_info")
    print("\n[ë©”íƒ€ ì •ë³´]")
    for key, value, updated_at in cursor.fetchall():
        print(f"  {key}: {value} ({updated_at[:10]})")

    # ì „ì²´ ë…¸íŠ¸ ìˆ˜
    cursor.execute("SELECT COUNT(*) FROM notes")
    total_notes = cursor.fetchone()[0]
    print(f"\n[ì „ì²´ ë…¸íŠ¸: {total_notes}ê°œ]")

    # í´ë” ì¹´í…Œê³ ë¦¬ë³„
    cursor.execute("""
        SELECT folder_category, COUNT(*) as cnt
        FROM notes
        GROUP BY folder_category
        ORDER BY cnt DESC
    """)
    print("\n[ì¹´í…Œê³ ë¦¬ë³„]")
    for category, cnt in cursor.fetchall():
        print(f"  {category}: {cnt}ê°œ")

    # ë…¸íŠ¸ ìœ í˜•ë³„
    cursor.execute("""
        SELECT note_type, COUNT(*) as cnt
        FROM notes
        WHERE note_type IS NOT NULL
        GROUP BY note_type
        ORDER BY cnt DESC
        LIMIT 10
    """)
    results = cursor.fetchall()
    if results:
        print("\n[ìœ í˜•ë³„ TOP 10]")
        for note_type, cnt in results:
            print(f"  {note_type}: {cnt}ê°œ")

    # ìƒìœ„ íƒœê·¸
    cursor.execute("""
        SELECT t.tag_name, COUNT(*) as cnt
        FROM tags t
        JOIN note_tags nt ON t.id = nt.tag_id
        GROUP BY t.tag_name
        ORDER BY cnt DESC
        LIMIT 10
    """)
    results = cursor.fetchall()
    if results:
        print("\n[íƒœê·¸ TOP 10]")
        for tag_name, cnt in results:
            print(f"  {tag_name}: {cnt}ê°œ")

    # ì €ìë³„
    cursor.execute("""
        SELECT author, COUNT(*) as cnt
        FROM notes
        WHERE author IS NOT NULL
        GROUP BY author
        ORDER BY cnt DESC
        LIMIT 10
    """)
    results = cursor.fetchall()
    if results:
        print("\n[ì €ì TOP 10]")
        for author, cnt in results:
            print(f"  {author}: {cnt}ê°œ")

    # ë§í¬ í†µê³„
    cursor.execute("SELECT COUNT(*) FROM links")
    total_links = cursor.fetchone()[0]
    print(f"\n[ë§í¬: {total_links}ê°œ]")

    # ê°€ì¥ ë§ì´ ì°¸ì¡°ë˜ëŠ” ë…¸íŠ¸
    cursor.execute("""
        SELECT target_note, COUNT(*) as cnt
        FROM links
        GROUP BY target_note
        ORDER BY cnt DESC
        LIMIT 5
    """)
    results = cursor.fetchall()
    if results:
        print("\n[ê°€ì¥ ë§ì´ ì°¸ì¡°ë˜ëŠ” ë…¸íŠ¸ TOP 5]")
        for target, cnt in results:
            print(f"  {target}: {cnt}íšŒ")

    conn.close()
    print("\n" + "=" * 50)


# ============================================================
# CLI
# ============================================================

def main():
    parser = argparse.ArgumentParser(description='ZK-PARA Vault ë©”íƒ€DB ê´€ë¦¬ (í•œê¸€ ì¸ì½”ë”© ì™„ì „ ì§€ì›)')
    parser.add_argument('--init', action='store_true', help='DB ì´ˆê¸°í™”')
    parser.add_argument('--sync', action='store_true', help='ì „ì²´ ë™ê¸°í™”')
    parser.add_argument('--verbose', '-v', action='store_true', help='ìƒì„¸ ì¶œë ¥ ëª¨ë“œ')
    parser.add_argument('--search', type=str, help='í‚¤ì›Œë“œ ê²€ìƒ‰')
    parser.add_argument('--tag', type=str, help='íƒœê·¸ë¡œ ê²€ìƒ‰')
    parser.add_argument('--category', type=str, help='ì¹´í…Œê³ ë¦¬ë¡œ ê²€ìƒ‰')
    parser.add_argument('--query', type=str, help='ì§ì ‘ SQL ì¿¼ë¦¬')
    parser.add_argument('--info', action='store_true', help='DB í†µê³„')

    args = parser.parse_args()

    if args.init:
        init_db()
    elif args.sync:
        sync_all(verbose=args.verbose)
    elif args.search:
        search(args.search)
    elif args.tag:
        search_by_tag(args.tag)
    elif args.category:
        search_by_category(args.category)
    elif args.query:
        execute_query(args.query)
    elif args.info:
        show_info()
    else:
        parser.print_help()


if __name__ == '__main__':
    main()
